@online{casper2011pkuthss,
	title={pkuthss: LaTeX template for dissertations in Peking University},
	url={https://gitea.com/CasperVector/pkuthss},
	author={Casper Ti. Vector},
	urldate={2011-06-26},
	year={2011}
}

@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	year={2019}
}

@inproceedings{yang2019xlnet,
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
	volume = {32},
	year = {2019}
}

@book{gut2013probability,
	title={Probability: a graduate course},
	author={Gut, Allan},
	volume={75},
	year={2013},
	publisher={Springer Science \& Business Media}
}

@book{liu2003uncertain,
  title={不确定规划及应用},
  author={刘宝碇},
  volume={1},
  year={2003},
  publisher={清华大学出版社有限公司}
}

@book{qiu2020nndl,
    title = {神经网络与深度学习},
    publisher = {机械工业出版社},
    year = {2020},
    author = {邱锡鹏},
    address = {北京},
    isbn = {9787111649687},
    url = {https://nndl.github.io/},
}

% vim:ts=4:sw=4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 作業內容 AI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 1
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

% 2
@online{githubtfws,
    author    = "TensorFlow",
    title     = "GitHub - tensorflow/tensor2tensor",
    url       = "https://github.com/tensorflow/tensor2tensor",
    keywords  = "tensorflow,tensor2tensor"
}

% 3
@online{pwcws,
    author    = "paperswithcode",
    title     = "paperswithcode - Attention Is All You Need",
    url       = "https://paperswithcode.com/paper/attention-is-all-you-need",
    keywords  = "papers,code"
}

% 4
@online{zhihuscir,
    author    = "哈工大 SCIR 赵正宇",
    title     = "ZhiHu - 搞懂 Transformer 结构，看这篇 PyTorch 实现就够了",
    url       = "https://zhuanlan.zhihu.com/p/339207092",
    keywords  = "transformer"
}

% 5
@online{githubtfmdws,
    author    = "TensorFlow",
    title     = "GitHub - tensorflow/models",
    url       = "https://github.com/tensorflow/models/tree/master/official/nlp/transformer",
    keywords  = "tensorflow,models"
}

% 6
@online{graykodemdws,
    author    = "graykode",
    title     = "GitHub - graykode/nlp-tutorial",
    url       = "https://github.com/graykode/nlp-tutorial",
    keywords  = "nlp"
}

% 7
@online{tfmrsle,
    author    = "Samuel Lynn-Evans",
    title     = "How to code The Transformer in Pytorch",
    url       = "https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec",
    keywords  = "transformer,pytorch"
}

% 8
@online{sletferws,
    author    = "graykode",
    title     = "GitHub - SamLynnEvans/Transformer",
    url       = "https://github.com/SamLynnEvans/Transformer",
    keywords  = "transformer"
}

% 9
@inproceedings{opennmt,
  author    = {Guillaume Klein and
               Yoon Kim and
               Yuntian Deng and
               Jean Senellart and
               Alexander M. Rush},
  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},
  booktitle = {Proc. ACL},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/P17-4012},
  doi       = {10.18653/v1/P17-4012}
}

% 10
@online{zhihutf10,
    author    = "ZhiHu",
    title     = "ZhiHu - 熬了一晚上，我从零实现了 Transformer 模型，把代码讲给你听",
    url       = "https://zhuanlan.zhihu.com/p/411311520",
    keywords  = "transformer"
}

% 11
@online{zhihutf11,
    author    = "ZhiHu",
    title     = "ZhiHu - This post is all you need（层层剥开Transformer）",
    url       = "https://zhuanlan.zhihu.com/p/420820453",
    keywords  = "transformer"
}

% 12
@online{hyltfr12,
    author    = "Hung-yi Lee",
    title     = "【機器學習2021】Transformer (上)",
    url       = "https://www.youtube.com/watch?v=n9TlOhRjYoc",
    keywords  = "transformer"
}

% 13
@online{hyltfr13,
    author    = "Hung-yi Lee",
    title     = "【機器學習2021】Transformer (下)",
    url       = "https://www.youtube.com/watch?v=N6aRv06iv2g",
    keywords  = "transformer"
}

% 14
@online{huggingfacemdws,
    author    = "huggingface",
    title     = "GitHub - huggingface/transformers",
    url       = "https://github.com/huggingface/transformers",
    keywords  = "transformer"
}

% 15
@online{harvardnlpatf,
    author    = "huggingface",
    title     = "GitHub - harvardnlp/annotated-transformer",
    url       = "https://github.com/harvardnlp/annotated-transformer",
    keywords  = "transformer"
}

% 16
@online{deepaksaini16,
    author    = "Deepak Saini",
    title     = "Transformer Implementation (Attention all you Need)",
    url       = "https://medium.com/analytics-vidhya/bert-implementation-multi-head-attention-4a10142636fe",
    keywords  = "transformer"
}

% 17
@online{fawazsammanitf17,
    author    = "fawazsammani",
    title     = "GitHub - fawazsammani/chatbot-transformer",
    url       = "https://github.com/fawazsammani/chatbot-transformer",
    keywords  = "transformer"
}

% 18
@online{deeplnewer18,
    author    = "DeepL Newer",
    title     = "Transformer 的 PyTorch 实现",
    url       = "https://wmathor.com/index.php/archives/1455/",
    keywords  = "transformer"
}

% 19
@online{zhihutf19,
    author    = "忆臻",
    title     = "ZhiHu - 搞懂 Transformer 结构，看这篇 PyTorch 实现就够了（上）",
    url       = "https://zhuanlan.zhihu.com/p/48731949",
    keywords  = "transformer"
}

% 20
@online{zhihutf20,
    author    = "ZhiHu",
    title     = "ZhiHu - 超详细图解 Self-Attention",
    url       = "https://zhuanlan.zhihu.com/p/410776234",
    keywords  = "transformer"
}

% 21
@online{zhihutf21,
    author    = "ZhiHu",
    title     = "ZhiHu - Transformer - Attention is all you need",
    url       = "https://zhuanlan.zhihu.com/p/311156298",
    keywords  = "transformer"
}

% 22
@article{pan2021integration,
  title={On the Integration of Self-Attention and Convolution},
  author={Pan, Xuran and Ge, Chunjiang and Lu, Rui and Song, Shiji and Chen, Guanfu and Huang, Zeyi and Huang, Gao},
  journal={arXiv preprint arXiv:2111.14556},
  year={2021}
}

% 23
@online{zhihutf23,
    author    = "CVer计算机视觉",
    title     = "ZhiHu - 又一篇视觉 Transformer 综述来了！",
    url       = "https://zhuanlan.zhihu.com/p/341995737",
    keywords  = "transformer"
}

% 24
@online{jackcui24,
    author    = "Jack Cui",
    title     = "Transformer 最新综述！",
    url       = "https://jishuin.proginn.com/p/763bfbd5f55d",
    keywords  = "transformer"
}

% 25
@online{mpweixinqq24,
    author    = "极市平台",
    title     = "Self-Attention 和 CNN 的优雅集成！清华大学等提出 ACmix，性能速度全面提升！",
    url       = "https://mp.weixin.qq.com/s/0LAYmXsGjxBwCm5roXFOtQ",
    keywords  = "transformer,cnn"
}

% 26
@online{deeplnewer26,
    author    = "DeepL Newer",
    title     = "Transformer 详解",
    url       = "https://wmathor.com/index.php/archives/1438/",
    keywords  = "transformer"
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 27
@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European Conference on Computer Vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@article{khan2021transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={arXiv preprint arXiv:2101.01169},
  year={2021}
}