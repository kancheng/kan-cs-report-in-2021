# About

0. Transformers in Vision: A Survey

1. Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks

2. Deep amortized clustering

3. Non-local Neural Networks

4. Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation

5. Taming Transformers for High-Resolution Image Synthesis

6. Video Action Transformer Network


# Transformers in Vision: A Survey

Transformer 模型在自然语言任务上的惊人结果引起了视觉界的兴趣，研究它们在计算机视觉问题中的应用，Transformer 与类似如长短期记忆 (LSTM)等的循环网络相比，Transformers 的显着优势之一是能够对输入序列元素之间的长期依赖关系进行建模，并支持序列的并行处理。 Transformer 与卷积网络不同，Transformer 的设计需要最小的归纳偏差，自然适合作为集合函数。此外 Transformers 的简单设计允许使用类似的处理块处理多种模态（例如，图像、视频、文本和语音），并展示了对超大容量网络和庞大数据集的出色可扩展性。这些优势使使用 Transformer 网络的许多视觉任务取得了令人兴奋的进展。本次调查旨在全面概述计算机视觉学科中的 Transformer 模型。研究者首先介绍 Transformers 成功背后的基本概念，即自我注意、大规模预训练和双向特征编码。然后，我们介绍了转换器在视觉中的广泛应用，包括如图像分类、对象检测、动作识别和分割等流行的识别任务领域，生成建模领域，又如视觉问答、视觉推理和视觉基础等多模态任务，再如活动识别、视频预测的视频处理领域，或者是图像超分辨率、图像增强和着色的 low-level vision 领域和点云分类和分割的 3D 分析。研究者比较了流行技术在架构设计和实验价值方面的各自优势和局限性。最后该研究对开放的研究方向和未来可能的工作进行了分析，此研究希望这项努力将进一步激发社区的兴趣，以解决当前在计算机视觉中应用变压器模型所面临的挑战。

Index Terms—Self-attention, transformers, 双向编码器(bidirectional encoders), deep neural networks, convolutional networks, self-supervision.

https://arxiv.org/abs/2101.01169


# Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, Yee Whye Teh

https://arxiv.org/abs/1810.00825

其原因在於許多機器學習任務，例如多實例學習、3D 形狀識別和小樣本圖像分類，都是在實例集上定義的，而由於此類問題的解決方案不依賴於集合元素的順序，用於解決這些問題的模型應該是置換不變，所以該研究提出了一個基於注意力的神經網絡模塊，即 Set Transformer，專門設計用於對輸入集中元素之間的交互進行建模。提出的模型由一個編碼器和一個解碼器組成，兩者都依賴於注意力機制，而為了降低計算複雜度，研究者引入了一種注意力機制，其靈感來自於從稀疏高斯過程文獻中引入點方法。它將自注意力的計算時間從集合中元素的數量的二次減少到線性。該研究表明的模型在理論上具有吸引力，並且在一系列任務上對其進行了評估，與最近的集合結構數據方法相比，表現出更高的性能。

該研究說明學習表徵已被證明是深度學習及其許多成功案例的基本問題，而深度學習解決的大多數問題都是基於實例的，並採用將固定維輸入張量映射到其相應目標值的形式 (Krizhevsky et al., 2012; Graves et al., 2013)，而對於某些應用程序，研究者認為會需要處理集合結構化數據。

再此研究者說明多實例學習 (Dietterich et al., 1997; Maron \& Lozano-Perez , 1998) 是這種集合輸入問題的一個例子，其中給定一組實例作為輸入，相應的目標是整套。
其他問題，例如 3D 形狀識別 (Wu et al.,2015; Shi et al., 2015; Su et al., 2015; Charles et al., 2017) 、序列排序 (Vinyals et al., 2016) 以及各種 集合操作（Muandet et al., 2012; Oliva et al., 2013; Edwards & Storkey, 2017; Zaheer et al., 2017）也可以被視為集合輸入問題，同時許多元學習 (Thrun & Pratt, 1998; Schmidhuber, 1987) 問題使用不同但相關的任務進行學習，也可以視為 setinput 任務，其中輸入集對應於單個任務的訓練數據集。

例如少樣本圖像分類 (Finn et al., 2017; Snell et al., 2017; Lee & Choi, 2018) 通過使用一組支持圖像構建分類器來運行，而該分類器使用查詢圖像進行評估，集合輸入問題的模型應該滿足兩個關鍵要求。首先第一個要求在於，它應該是排列不變的——模型的輸出在輸入集中元素的任何排列下都不應該改變，其次此類模型應該能夠處理任何大小的輸入集。雖然這些要求源於集合的定義，但它們在基於神經網絡的模型中並不容易滿足：經典的前饋神經網絡違反了這兩個要求，並且 RNN 對輸入順序很敏感，而近來 Edwards & Storkey (2017) and Zaheer et al. (2017)  提出了滿足這兩個標準的神經網絡架構，我們稱之為集合池方法，在此模型中在集合中的每個元素首先被獨立地饋送到接受固定大小輸入的前饋神經網絡。然後使用池化操作 (均值(mean), 總和(sum), 最大值(max) or 類似(similar)).聚合得到的特徵空間嵌入，而通過對聚合嵌入的進一步非線性處理獲得最終輸出。
這種非常簡單的架構滿足上述兩個要求，更重要的是它被證明是任何集合函數的通用逼近器 (Zaheer et al., 2017)。由於此類特性可以根據黑盒方式學習輸入集與其目標輸出之間的複雜映射，就像前饋或循環神經網絡一樣，儘管這種集合池方法在理論上很有吸引力，但研究者仍不清楚我們是否可以僅使用基於實例的特徵提取器和簡單的池操作來很好地近似複雜的映射。
由於集合中的每個元素都在集合池操作中獨立處理，因此必須丟棄一些有關元素之間交互的信息，這可能會使一些問題不必要地難以解決。考慮攤銷聚類的問題，研究者會想學習從輸入點集到該集內點簇中心的參數映射，即使面對於二維空間中的玩具數據集，這也不是一個容易的問題。

主要的困難在於參數映射必須在對解釋模式進行建模時將每個點分配給其相應的集群，這樣得到的集群就不會試圖解釋輸入集的重疊子集。由於這種先天的困難，聚類通常通過迭代算法來解決，這些算法細化隨機初始化的聚類直到收斂。
儘管具有集合極化操作的神經網絡可以通過學習量化空間來近似這種攤銷映射，但一個關鍵的缺點是這種量化不能依賴於集合的內容。
這限制了解決方案的質量，也可能使此類模型的優化更加困難；研究者在該研究名為 Experiments 的第 5 節，讓我們知道經驗表明，此類池化架構存在欠擬合的問題。
該研究提出了一種稱為 Set Transformer 的新型集輸入深度神經網絡架構(cf. Transformer, (Vaswani et al., 2017))。 而 Set Transformer 的新穎之處在於三個重要的設計選擇：

1. 研究使用自注意力機制來處理輸入集中的每個元素，這允許我們的方法自然地編碼集中元素之間的成對或高階交互。

2. 研究提出了一種將完全自注意力 (e.g. the Transformer) 的 $O(n^{2})$ 計算時間減少到 $O(nm)$ 的方法，其中 m 是固定的超參數，允許該研究的方法擴展到大型輸入集。

3. 研究使用自註意力機制來聚合特徵，這在問題需要多個相互依賴的輸出時特別有用，例如元聚類問題 (the problem of meta-clustering)，其中每個聚類中心的含義在很大程度上取決於其相對位置到其他集群。

該研究將 Set Transformer 應用於幾個集合輸入問題，並憑經驗證明了這些設計選擇的重要性和有效性，並表明我們可以為大多數任務實現最先進的性能。



# Deep amortized clustering

Juho Lee, Yoonho Lee, Yee Whye Teh

https://arxiv.org/abs/1909.13433

研究者們提出了一種深度攤銷聚類 (DAC; deep amortized clustering)，這是一種神經架構，它學習使用一些前向傳播有效地對數據集進行聚類，其 DAC 隱式地學習是什麼構成了一個集群，如何將數據點分組到集群中，以及如何計算數據集中的集群數量。而 DAC 是使用標記數據集進行元學習的，這一過程不同於傳統的聚類算法，而傳統聚類算法通常需要手工指定的關於聚類形狀和結構的先驗知識。研究者憑經驗表明，在合成數據和圖像數據上，DAC 可以有效且準確地對來自用於生成訓練數據集的相同分佈的新數據集進行聚類。
而從圖可知，該研究的模型每次迭代識別一個集群（頂部），使其能夠找到任意數量的集群（底部），其聚類是無監督機器學習中的一項基本任務，用於將相似的數據點分組到多個集群中。除了在許多下游任務中的有用性之外，聚類是可視化和理解數據集底層結構的重要工具，也是認知科學中的分類模型，而大多數聚類算法有兩個基本組成部分——如何定義集群以及如何將數據點分配給這些集群。

前者通常使用度量來測量數據點之間的距離，或使用描述集群形狀的生成模型來定義，而後者如何將數據點分配給集群，然後通常會迭代優化 w.r.t. 基於集群定義導出的目標函數。
請注意聚類定義是用戶定義的，是用戶對聚類過程的先驗知識的反映，不同的定義導致不同的聚類，然而實踐中使用的集群定義通常非常簡單，例如 k-means 中的集群是根據到質心的 2 距離定義的，而高斯是混合模型中集群的常用生成模型。近來深度學習的進步促進了以黑盒方式逼近複雜函數，本研究中與聚類問題相關的一個特殊應用是攤銷推理 (Gershman & Goodman, 2014; Stuhlmüller et al., 2013)，其中訓練神經網絡以預測潛在變量的狀態模型或概率程序。而在學習集輸入神經網絡的背景下 (Zaheer et al., 2017)，Lee et al. (2019)  表明可以分攤高斯混合 (MOG;a Mixture of Gaussians) 的迭代聚類過程，而 Pakman et al. (2019) 證明可以訓練神經網絡將數據點順序分配給集群。此兩種方法都可以解釋為使用神經網絡對給定數據集的集群分配和參數進行攤銷推理，而在此要注意的部分在於，一旦將神經網絡用於攤銷聚類，研究者們就可以利用它們的靈活性來使用更複雜的方法來定義聚類。此外，攤銷網絡可以使用生成的數據集進行訓練，其中地面實況聚類是已知的。這也可以解釋為隱式學習訓練數據集底層的集群定義，這樣攤銷推理（大約）會產生適當的集群。

從某種意義上來看，這與神經過程 (Garnelo et al., 2018b;a)有著相似的哲學，後者從多個數據集進行元學習 (meta-learns) 以學習函數的先驗，回到該研究來看，研究者以這些先前的工作為基礎，並提出了深度攤銷聚類 (DAC;Deep Amortized Clustering)，該研究與之前的工作一樣，DAC 中的攤銷網絡使用生成的數據集進行訓練，其中地面實況聚類是已知的，像 Lee et al. (2019)，DAC 使用 Set Transformer，但與 Lee et al. (2019) 因為它按順序生成集群，這可以根據數據集的複雜性生成不同數量的集群。此研究的方法還擴展了 Lee et al. (2019) 從 MOG 到更複雜的集群定義問題，可以說這些問題更難手動指定，更容易從數據中進行元學習，而其工作也不同於 Pakman et al.(2019)，因為我們的網絡並行處理數據點，而 Pakman et al. (2019)按順序處理它們，這可以說是可擴展性較低，並且限制了對較小數據集的適用性。

該研究的組織如下。研究者們首先在該研究的名為 A PRIMER ON SET TRANSFORMER AND AMORTIZED CLUSTERING 的第 2 節中，去描述在整篇論文中使用的置換不變集變換器模塊。，同時在名為 DEEP AMORTIZED CLUSTERING 的第 3 節中，此研究描述了研究者們如何實現一次識別一個集群的核心思想，並描述了研究者的聚類框架 DAC 在復雜數據集上解決 DAC 有幾個挑戰，同時按難度順序構建此論文 。



# Non-local Neural Networks

Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He

https://arxiv.org/abs/1711.07971

此研究指出卷積和循環操作都是一次處理一個局部鄰域的構建塊。在該研究中，研究者們將非本地操作呈現為用於捕獲遠程依賴項的通用構建塊系列，而受計算機視覺中經典的非局部均值方法的啟發，研究者的非局部操作將某個位置的響應計算為所有位置特徵的加權和，而此構建塊可以插入到許多計算機視覺架構中。同時在視頻分類任務上，即使沒有任何繁雜的東西，研究者的非本地模型也可以在 Kinetics 和 Charades 數據集上與當前的競爭者競爭或勝出，另外在靜態圖像識別中，研究者們的非局部模型改進了 COCO 任務套件上的對象檢測/分割和姿態估計，可捕獲遠程依賴關係在深度神經網絡中至關重要。對於如在語音(speech)、語言(language) 的循環操作(recurrent operations) 與序列數據(sequential data) 是遠程依賴建模的主要解決方案，而對於圖像數據來說，長距離依賴是由卷積運算的深堆棧形成的大感受野建模。另外卷積運算和循環運算都在空間或時間上處理局部鄰域， 因此只有在重複應用這些操作時才能捕獲遠程依賴關係，通過數據逐步傳播信號。在此重複本地操作有幾個限制，首先它在計算上效率低下，其次它會導致需要仔細解決的優化困難。

最後這些挑戰使得多跳依賴建模變得困難，比如當需要在遠距離位置之間來回傳遞消息的時候。在該研究中，研究者將非本地操作作為一種高效、簡單且通用的組件，用於捕獲深度神經網絡的遠程依賴關係，當中所提出的非局部操作是計算機視覺中經典非局部平均操作的推廣。直觀地說，非局部操作將某個位置的響應計算為輸入特徵圖中所有位置的特徵的加權。此組位置可以是空間、時間或時空，這意味著我們的操作適用於圖像、序列和視頻問題。使用非本地操作有幾個優點：(a) 與循環和卷積運算的漸進行為相反，非局部運算通過計算任意兩個位置之間的相互作用直接捕獲遠程依賴關係，而不管它們的位置距離如何；(b) 正如我們在實驗中所展示的，即使只有幾層，非本地操作也是有效的，並且可以達到最佳效果；(c) 最後，研究非本地操作保持了可變的輸入大小，並且可以很容易地與其他操作，例如研究者將使用的卷積結合起來，同時研究展示了非本地操作在視頻分類應用中的有效性，而在視頻中遠距離的空間和時間像素之間會發生遠程交互。

作為該研究的基本單元的單個非本地塊可以前饋方式直接捕獲這些時空依賴性，有了一些非局部塊，研究者認為非局部神經網絡且包含包括膨脹變體的架構對於視頻分類比 2D 和 3D 卷積網絡更為準確。此外，非局部神經網絡比 3D 卷積神經網絡在計算上更經濟，同時在 Kinetics 和 Charades 數據集上介紹了綜合消融研究。僅使用 RGB 而沒有任何類似光流、多尺度測試複雜的東西，本研究的方法在兩個數據集上取得的結果與最新的比賽獲勝者相當或更好。為了證明非局部操作的普遍性，研究進一步介紹了 COCO 數據集上的對象檢測/分割和姿態估計實驗，而在強大的 Mask R-CNN 基線 之上，研究者的非局部塊可以以很小的額外計算成本提高所有三個任務的準確性，連同視頻中的證據，這些圖像實驗表明非局部操作通常是有用的，並且可以成為設計深度神經網絡的基本構建塊。從圖中我們可以看到網絡中的一個時空非局部操作訓練用於 Kinetics 中的視頻分類，位置 $x_{i}$ 的響應由所有位置 $x_{j}$ 的特徵的加權平均值計算，而此處僅顯示最高權重的特徵，在這個由我們的模型計算的示例中，請注意它如何將第一幀中的球與最後兩幀中的球相關聯。



# Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation

Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh Chen

https://arxiv.org/abs/2003.07853

卷積利用局部性來提高效率，代價是丟失了遠程上下文，已採用自註意力來增強具有非本地交互作用的 CNN，最近的工作證明可以通過將注意力限制在局部區域來堆疊自註意力層以獲得完全注意力網絡，該研究試圖通過將 2D 自註意力分解為兩個一維自註意力來消除此約束，在此降低了計算複雜度，並允許在更大甚至全局區域內執行注意力。同時研究者還提出了一種位置敏感的自注意力設計，將兩者結合產生研究者想要的位置敏感軸向注意層，這是一種新穎的構建塊，可以堆疊以形成用於圖像分類和密集預測的軸向注意模型。該研究證明了研究者的模型在四個大規模數據集上的有效性，特別在於其模型優於 ImageNet 上所有現有的獨立自注意力模型，而研究者的 Axial-DeepLab 在 COCO 測試開發上比自下而上的最新技術提高了 2.8% 的 PQ，先前的最新技術是通過我們的小變體實現的，該變體的參數效率為 3.8 倍，計算效率為 27 倍，Axial-DeepLab 還在 Mapillary Vistas 和 Cityscapes 上取得了最先進的結果。

卷積是計算機視覺的核心構建塊，早期算法使用卷積濾波器來模糊圖像、提取邊緣或檢測特徵。前者與全連接模型相比，由於其效率和泛化能力，它在現代神經網絡中得到了大量利用。卷積的成功主要來自平移等方差性和局部性的兩個特性，平移等方差雖然不精確，但與成像的性質非常吻合，因此可以將模型推廣到不同的位置或不同尺寸的圖像。另一方面，局部性減少了參數計數和 M-Adds，然而它使建模遠程關係具有挑戰性，另外大量文獻討論了在卷積神經網絡 (CNN) 中對遠程交互進行建模的方法，而一些採用多孔卷積、更大的內核或圖像金字塔，要么是手工設計的，又或者是通過算法搜索，另一行作品採用注意力機制，注意顯示了它在語言建模、語音識別和神經字幕中對遠程交互進行建模的能力。此後注意力已擴展到視覺，顯著提升了圖像分類、對象檢測、語義分割、視頻分類和對抗性防禦。這些工作通過非局部或遠程注意力模塊豐富了 CNN，近來有研究已經提出將注意力層堆疊為沒有任何空間卷積的獨立模型並顯示出有希望的結果，同時樸素的注意力在計算上是昂貴的，尤其是在大輸入上。 提出的將局部約束應用於注意力，可以降低成本並能夠構建完全注意力模型。同時局部約束限制了模型感受野，這對於分割等任務至關重要，尤其是在高分辨率輸入上。在此項工作中，我們建議採用軸向注意力，它不僅可以進行高效計算，而且可以恢復獨立注意力模型中的感受。


核心思想是將 2D 注意力分解為沿高度和寬度軸順序的兩個 1D 注意力，其效率使我們能夠參與大區域並構建模型來學習遠程甚至全局交互。此外，大多數先前的注意力模塊不利用位置信息，這會降低如多尺度的形狀或對象的注意力在建模與位置相關的交互中的能力。最近的作品會將位置術語引入註意力，但是以一種與上下文無關的方式。在該研究中，研究者將位置項增加為上下文相關，使研究的注意力對位置敏感，並具有邊際成本。研究展示該研究的軸向注意力模型在 ImageNet 上進行分類的有效性，以及在三個數據集（COCO 、Mapillary Vistas 和 Cityscapes ）上進行全景分割、實例分割、和語義分割。特別是，在 ImageNet 上，研究者通過用其位置敏感的軸向注意層替換所有殘差塊中的 3 × 3 卷積來構建 Axial-ResNet，並通過採用軸向注意層進一步使其獲得完全注意。
因此該研究的 Axial-ResNet 在 ImageNet 上的獨立注意力模型中取得了最先進的結果。而對於分割任務，研究者通過替換 Panoptic-DeepLab 中的主干將 Axial-ResNet 轉換為 Axial-DeepLab。


在 COCO 上，研究者的 Axial-DeepLab 在測試開發集上以 2.8% 的 PQ 優於當前自下而上的最新技術 Panoptic-DeepLab，而研究者還在 Mapillary Vistas 和 Cityscapes 上展示了最先進的分割結果。

總而言之，研究者的貢獻有四方面：

– 所提出的方法是首次嘗試構建具有大或全局感受野的獨立注意力模型。

– 該研究提出了位置敏感的注意力層，它可以更好地利用位置信息而不會增加太多的計算成本。

– 該研究表明軸向注意力運作良好，不僅作為圖像分類的獨立模型，而且作為全景分割、實例分割和分割分割的支柱。

– 該研究的 Axial-DeepLab 在 COCO 上比自下而上的最新技術顯著改進，實現了與兩階段方法相當的性能。

最後此研究還在 Mapillary Vistas 和 Cityscapes 上超越了之前最先進的方法。



# Taming Transformers for High-Resolution Image Synthesis

Patrick Esser, Robin Rombach, Björn Ommer

https://arxiv.org/abs/2012.09841

研究首先指出在學習序列數據上的遠程交互，transformers 繼續在各種任務上顯示出最先進的結果，而前者與 CNN 相比，它們不包含優先考慮局部交互的歸納偏差。這使它們具有表現力，但對於如高分辨率圖像的長序列，在其計算上也不可行。
本研究展示瞭如何將 CNN 的歸納偏置的有效性與 Transformer 的表達能力相結合，使它們能夠建模並從而合成高分辨率圖像，大致分為 (i) 使用 CNN 來學習圖像成分的上下文豐富的詞彙，並反過來 (ii) 利用轉換器在高分辨率圖像中有效地對其組成進行建模。

該研究的方法很容易應用於條件合成任務，其中如對像類等非空間信息和如分割等空間信息都可以控制生成的圖像，特別在於此研究展示了帶有轉換器的百萬像素圖像的語義引導合成的第一個結果，並在類條件 ImageNet 上的自回歸模型中獲得了最先進的技術。

Transformer 正在興起——它們現在是語言任務的事實上的標準架構，並且越來越多地適用於其他領域，例如音頻和視覺。前者與主要的視覺架構卷積神經網絡 (CNN) 相比，transformer 架構不包含有關交互位置的內置歸納先驗，因此可以自由地學習其輸入之間的複雜關係，然而此種普遍性也意味著它必須學習所有關係，而 CNN 旨在利用有關圖像內強局部相關性的先驗知識。因此 transformer 的表現力的增加伴隨著計算成本的二次增加，因為所有的成對交互都被考慮在內，同時最先進的變壓器模型所產生的能量和時間要求為將它們縮放為具有數百萬像素的高分辨率圖像帶來了根本問題。

變壓器傾向於學習卷積結構的觀察結果因此提出了一個問題：

每次訓練視覺模型時，研究者考量到是否必須從頭開始重新學習我們所知道的關於圖像的局部結構和規律性的所有內容，或者研究者是否可以有效地編碼歸納圖像偏差，同時仍然保留轉換器的靈活性的部分，該研究假設低級圖像結構可以通過局部連接很好地描述，即卷積架構，而這種結構假設在更高的語義級別上不再有效。此外 CNN 不僅表現出強烈的局部性偏差，而且通過在所有位置上使用共享權重，還表現出對空間不變性的偏差，但如果需要更全面地了解輸入，這會使它們無效。

研究者獲得有效且富有表現力的模型的關鍵見解是，卷積和變換器架構一起可以模擬我們視覺世界的組成性質，該研究使用卷積方法有效地學習上下文豐富的視覺部分的碼本，然後學習它們的全局組合模型。而這些組合中的遠程交互需要一個富有表現力的轉換器架構來模擬其組成視覺部分的分佈。此外，研究者利用對抗性方法來確保局部部分的字典捕獲感知上重要的局部結構，以減輕使用轉換器架構對低級統計數據進行建模的需要。讓 Transformer 專注於它們獨特的優勢——對遠程關係建模——使它們能夠生成如圖中所示的高分辨率圖像，這是以前無法實現的壯舉。其圖中表明此研究的方法使 Transformer 能夠合成像這樣的高分辨率圖像，其中包含 1280x460 像素，同時該研究的公式通過調節有關所需對像類別或空間佈局的信息來控制生成的圖像。最後實驗表明，該研究的方法通過優於以前基於卷積架構的基於碼本的最新方法，保留了轉換器的優勢。



# Video Action Transformer Network

Rohit Girdhar, João Carreira, Carl Doersch, Andrew Zisserman

https://arxiv.org/abs/1812.02707

本研究引入了 Action Transformer 模型，用於識別和定位視頻剪輯中的人類動作，該研究重新利用了 Transformer 風格的架構，從研究中試圖對其行為進行分類的人周圍的時空上下文中聚合特徵。研究表明，通過使用高分辨率、特定於個人、與類別無關的查詢，該模型自發地學習跟踪個人並從他人的行為中獲取語義上下文。此外，它的注意力機制學習強調手和臉，這對於區分動作通常是至關重要的——除了框和類別標籤之外，所有這些都沒有明確的監督。該研究在 Atomic Visual Actions (AVA) 數據集上訓練和測試研究本身的 Action Transformer 網絡，僅使用原始 RGB 幀作為輸入，其性能明顯優於最新技術。在此研究中，研究者的目標是定位和識別視頻剪輯中的人類行為，人類行為仍然如此難以識別的一個原因是，推斷一個人的行為通常需要了解周圍的人和物體。比如識別一個人是否在“聽某人說話”取決於場景中是否存在另一個人在說些什麼。同樣要識別一個人是“指著一個物體”，還是“拿著一個物體”，還是“握手”；所有這些都需要對人及其周圍環境的有生命和無生命元素進行共同推理。

所以這不僅限於給定時間點的上下文而已，從識別“看人”的動作，在被看人走出畫面後，需要隨著時間的推移進行推理，以了解我們感興趣的人實際上是在看人，而不僅僅是凝視遠方。因此，該研究尋求一種在確定感興趣的人的行為時可以確定和利用此類上下文信息（其他人、其他對象）的模型，Vaswani et al. 的 Transformer 架構是一個合適的模型，因為它使用自注意力明確地為其表示構建上下文支持。在此與傳統的循環模型相比，這種架構在序列建模任務方面取得了巨大成功。同時另一個問題是在於，如何為人類行為識別建立一個類似的模型？其研究者的的答案是一個新的視頻動作識別網絡 Action Transformer，它使用修改後的 Transformer 架構作為“頭部”來對感興趣的人的動作進行分類。它匯集了另外兩個想法：(i) 一個時空 I3D 模型，該模型在以前的視頻動作識別方法中取得了成功，這項已提供了基本特徵； (ii) 區域提議網絡 (RPN) ，此項提供了一種用於定位執行動作的人的採樣機制。I3D 特徵和 RPN 一起生成查詢，該查詢是 Transformer 頭的輸入，該查詢聚合來自周圍視頻中其他人和對象的上下文信息。研究者除了詳細描述了這種架構也表明，經過訓練的網絡能夠學習跟踪個人並根據視頻中其他人的行為將他們的行為置於情境中。此外，轉換器關注手部和面部區域，研究者已經知道它們在區分動作時具有一些最相關的特徵。

所有這些都是在沒有明確監督的情況下獲得，而是在動作分類期間學習，並且在 Atomic Visual Actions (AVA) 數據集上訓練和測試研究的模型，而對於這種上下文推理，這是一個有趣且合適的測試平台，它需要在視頻中半密集地檢測多個人，並識別多個基本動作。許多這些動作通常不能僅從人物邊界框確定，而是需要推斷與其他人和物體的關係，前者與之前的作品不同，研究者的模型無需明確的對象檢測即可學會這樣做。研究者在 AVA 數據集上創造了新記錄，將性能從 17.4% 提高到 25.0% mAP。該網絡僅使用原始 RGB 幀，但其性能優於之前的所有工作，包括使用額外光流和聲音輸入的大型集成。在提交時，該研究的方法是 ActivityNet 排行榜上表現最好的方法然而研究者也注意到在 25% mAP 時，這個問題，甚至這個數據集，還遠未解決。因此，此研究最後嚴格分析了模型的失敗案例，該研究描述了一些常見的故障模式，並分析了按語義和空間標籤分解的性能。

同時研究者發現許多訓練集相對較大的類仍然難以識別，並調查此類尾部案例以標記未來工作的潛在途徑。而從圖中可以知道，操作中的 Action Transformer，研究者提出的多頭/層 Action Transformer 架構學習關注感興趣的人的相關區域及其上下文（其他人、對象），以識別他們正在做的動作。每個頭部計算一個剪輯嵌入，用於關注不同的部分，如面部、手和其他人，以識別感興趣的人正在“牽手”和“看著一個人”。


