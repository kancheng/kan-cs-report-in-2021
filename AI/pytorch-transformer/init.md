# AI - 人工智慧

> 2101212850 干皓丞

PKU 2021 個人實驗報告作業

## 0. About

Transformer 模型根據 Attention Is All You Need 進行實現。

## 實現 Code


## Reference

1. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, Attention Is All You Need, https://arxiv.org/abs/1706.03762

2. https://github.com/tensorflow/tensor2tensor

3. https://paperswithcode.com/paper/attention-is-all-you-need

4. https://zhuanlan.zhihu.com/p/339207092

5. https://github.com/tensorflow/models/tree/master/official/nlp/transformer

6. https://github.com/graykode/nlp-tutorial

7. https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec

8. https://github.com/SamLynnEvans/Transformer

9. http://nlp.seas.harvard.edu/2018/04/03/attention.html

10. https://zhuanlan.zhihu.com/p/411311520

11. https://zhuanlan.zhihu.com/p/420820453

12. https://www.youtube.com/watch?v=n9TlOhRjYoc

13. https://www.youtube.com/watch?v=N6aRv06iv2g

14. https://github.com/huggingface/transformers

15. https://github.com/harvardnlp/annotated-transformer

16. https://medium.com/analytics-vidhya/bert-implementation-multi-head-attention-4a10142636fe

17. https://github.com/fawazsammani/chatbot-transformer

18. https://wmathor.com/index.php/archives/1455/

19. https://zhuanlan.zhihu.com/p/48731949


