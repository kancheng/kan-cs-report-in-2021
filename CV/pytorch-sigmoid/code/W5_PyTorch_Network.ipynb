{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现两层全连接神经网络\n",
    "--------------\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。\n",
    "- ##  $h = W_1X$\n",
    "- ## $h_{relu} = max(0, h)$\n",
    "- ## $y_{pred} = W_2 h_{relu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案一：\n",
    "\n",
    "## 用 numpy 实现两层神经网络\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32760332.78537062\n",
      "50 18453.703819797192\n",
      "100 912.7155486709695\n",
      "150 73.30407075891168\n",
      "200 7.2165139702279895\n",
      "250 0.8062327538551376\n",
      "300 0.0981733647629658\n",
      "350 0.012704385438356888\n",
      "400 0.0017174817983649658\n",
      "450 0.00023961750375020837\n",
      "500 3.419821523577262e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    h = x.dot(w1) # N * H\n",
    "    h_relu = np.maximum(h, 0) # N * H\n",
    "    y_pred = h_relu.dot(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案二：\n",
    "\n",
    "## PyTorch: Tensors 实现两层神经网络\n",
    "\n",
    "使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38362032.0\n",
      "50 19347.6875\n",
      "100 1365.721435546875\n",
      "150 180.57022094726562\n",
      "200 29.791967391967773\n",
      "250 5.396778583526611\n",
      "300 1.025632619857788\n",
      "350 0.20053671300411224\n",
      "400 0.04009748995304108\n",
      "450 0.008369175717234612\n",
      "500 0.0020418709609657526\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    h = x.mm(w1) # N * H\n",
    "    h_relu = h.clamp(min=0) # N * H\n",
    "    y_pred = h_relu.mm(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案三：\n",
    "\n",
    "## PyTorch: Tensors 和 Autograd 实现两层神经网络\n",
    "\n",
    "\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果``x``是一个Tensor并且``x.requires_grad=True``那么``x.grad``是另一个储存着``x``当前梯度(相对于一个scalar，常常是loss)的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33255704.0\n",
      "50 10729.2197265625\n",
      "100 298.8142395019531\n",
      "150 14.81286334991455\n",
      "200 0.9232968688011169\n",
      "250 0.06418387591838837\n",
      "300 0.004952050279825926\n",
      "350 0.0005932210478931665\n",
      "400 0.0001456607860745862\n",
      "450 5.7708843087311834e-05\n",
      "500 3.1205090635921806e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum() # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案四：\n",
    "\n",
    "## PyTorch: Tensors 和 optim 实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31978560.0\n",
      "50 13783.6044921875\n",
      "100 616.9820556640625\n",
      "150 49.179222106933594\n",
      "200 4.8297014236450195\n",
      "250 0.522691011428833\n",
      "300 0.05973578989505768\n",
      "350 0.007287117652595043\n",
      "400 0.0011213204124942422\n",
      "450 0.00027184493956156075\n",
      "500 9.802633576327935e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum() # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "#     with torch.no_grad():\n",
    "#         w1 -= learning_rate * w1.grad\n",
    "#         w2 -= learning_rate * w2.grad\n",
    "#         w1.grad.zero_()\n",
    "#         w2.grad.zero_()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案五：\n",
    "\n",
    "## PyTorch: Tensors 和 nn.MSELoss 实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32740072.0\n",
      "50 17535.12109375\n",
      "100 841.505126953125\n",
      "150 64.40705108642578\n",
      "200 5.932732105255127\n",
      "250 0.6023210883140564\n",
      "300 0.06497772783041\n",
      "350 0.007521670311689377\n",
      "400 0.0011529145995154977\n",
      "450 0.0002922675048466772\n",
      "500 0.0001107300486182794\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    # loss = (y_pred - y).pow(2).sum() \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案六：\n",
    "\n",
    "## PyTorch: nn 实现两层神经网络\n",
    "\n",
    "使用PyTorch中nn这个库来构建网络。\n",
    "用PyTorch autograd来构建计算图和计算gradients，\n",
    "然后PyTorch会帮我们自动计算gradient。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30020408.0\n",
      "50 11235.6328125\n",
      "100 292.935791015625\n",
      "150 12.274823188781738\n",
      "200 0.6340152025222778\n",
      "250 0.036800604313611984\n",
      "300 0.002526442054659128\n",
      "350 0.0003348247555550188\n",
      "400 9.150088590104133e-05\n",
      "450 3.98082411265932e-05\n",
      "500 2.1767235011793673e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=True), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=True),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    \n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters(): # param (tensor, grad)\n",
    "            param -= learning_rate * param.grad\n",
    "#             param.grad.zero_()\n",
    "            \n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案七：\n",
    "\n",
    "## PyTorch: nn 和 Optim 实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28515726.0\n",
      "50 13081.2568359375\n",
      "100 541.2086791992188\n",
      "150 39.39814758300781\n",
      "200 3.4956207275390625\n",
      "250 0.33633261919021606\n",
      "300 0.03371109813451767\n",
      "350 0.0036836632061749697\n",
      "400 0.0005850918241776526\n",
      "450 0.00015891357907094061\n",
      "500 6.251141167012975e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "# learning_rate = 1e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案八：\n",
    "\n",
    "## PyTorch:  自定义 nn Modules 实现两层神经网络 (显式参数)\n",
    "\n",
    "可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.1352224349975586\n",
      "50 0.38237589597702026\n",
      "100 0.05351356416940689\n",
      "150 0.015370173379778862\n",
      "200 0.008596157655119896\n",
      "250 0.005803755018860102\n",
      "300 0.003925961442291737\n",
      "350 0.0024989633820950985\n",
      "400 0.0014968174509704113\n",
      "450 0.0008101157727651298\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define the model architecture\n",
    "        self.W1 = nn.Parameter(nn.init.xavier_normal_(torch.Tensor(D_in, H)))\n",
    "        self.W2 = nn.Parameter(nn.init.xavier_normal_(torch.Tensor(H, D_out)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = x.mm(self.W1).clamp(min=0).mm(self.W2)\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "# loss_fn = nn.MSELoss(reduction='sum')\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案九：\n",
    "\n",
    "## PyTorch: 自定义 nn Modules 实现两层神经网络 (隐式参数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 604.010009765625\n",
      "50 160.0804901123047\n",
      "100 33.51659393310547\n",
      "150 3.980309247970581\n",
      "200 0.2226010411977768\n",
      "250 0.006924773100763559\n",
      "300 0.0001677029358688742\n",
      "350 4.555347913992591e-06\n",
      "400 1.1379442099723747e-07\n",
      "450 2.3033137619421495e-09\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define the model architecture\n",
    "        self.linear1 = torch.nn.Linear(D_in, H, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kan Horst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
